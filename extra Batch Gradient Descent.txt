import numpy as np

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score
iris = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

X_train_bias = np.c_[np.ones((len(X_train), 1)), X_train] 
X_test_bias = np.c_[np.ones((len(X_test), 1)), X_test]
def softmax(logits):
    exp_logits = np.exp(logits)

    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True) 
    return exp_logits / sum_exp_logits
n_inputs = X_train_bias.shape[1] 
n_outputs = len(np.unique(y_train))
theta = np.random.randn(n_inputs, n_outputs)

eta = 0.1
n_epochs = 1000

max_epochs_without_improvement = 50 
best_loss = np.inf
epochs_without_improvement = 0
 
for epoch in range(n_epochs):
    logits_train = X_train_bias.dot(theta)
    y_proba_train = softmax(logits_train)

  
    loss_train = -np.mean(np.sum(np.log(y_proba_train) * (y_train.reshape(-1,1) == np.arange(n_outputs)), axis=1))

    error_train = y_proba_train - (y_train.reshape(-1, 1) == np.arange(n_outputs))
    grad = 1/len(X_train_bias) * X_train_bias.T.dot(error_train)
    theta -= eta * grad
    logits_test = X_test_bias.dot(theta)
    y_proba_test = softmax(logits_test) 

    loss_test = -np.mean(np.sum(np.log(y_proba_test) * (y_test.reshape(-1, 1)== np.arange(n_outputs)), axis=1))
    if loss_test < best_loss:
        best_loss = loss_test 
        epochs_without_improvement = 0
    else:

          epochs_without_improvement += 1 
    if epoch % 50 == 0:
 
         print("Epoch:", epoch, "Loss(train):", loss_train, "Loss(test):",loss_test) 
if epochs_without_improvement > max_epochs_without_improvement: print("Early stopping!")
break

logits_test = X_test_bias.dot(theta)
y_proba_test = softmax(logits_test)
y_pred_test = np.argmax(y_proba_test, axis=1)
accuracy = accuracy_score(y_test, y_pred_test) 
print("Accuracy:", accuracy)
